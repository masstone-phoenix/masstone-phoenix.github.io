<!DOCTYPE HTML>
<html>
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
    <meta name="Keywords" content="blog"/>
    <meta name="Description" content="blog"/>
    <title>Simple</title>
    <link rel="shortcut icon" href="/static/favicon.png"/>
    <link rel="stylesheet" type="text/css" href="/main.css" />
</head>
<body>
<div class="main">
    <div class="header">
    	<ul id="pages">
            <li><a href="/">home</a></li>
            <li><a href="/#/tags">tags</a></li>
            <li><a href="/#/archive">archive</a></li>
    	</ul>
    </div>
	<div class="wrap-header">
	<h1>
    <a href="/" id="title"></a>
	</h1>
	</div>
<div id="md" style="display: none;">
<!-- markdown -->
学习这个的目的是因为想使用机器学习的模型在手机（iOS，android）上使用一个模型进行预测，而且想要使模型尽可能的快，最好能做到实时，这样的话用上GPU就是必需的了，CPU基本上很难做到实时要求的，虽然这不是强需要，但是能满足的话为什么不呢？这样的话经过我们的调研，tensorflow可以通过tfcoreml工具直接转到苹果的Core ML上，而这个框架是支持GPU的；在android上，tensorflow通过调用的The Android Neural Networks API (NNAPI)的方式来支持GPU。

但是，经过我们的实际使用，在iOS这边，tensorflow mobile的速度不容乐观。这里只是记录一下如何让其在iOS上跑起来的，里面的坑还是挺多的。

# 搭建模型




# 部署到iOS
先按照[官方文档](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/ios#building-the-tensorflow-ios-libraries-from-source)编译tensorflow库，加入到Xcode工程中，调用如下：

```
+ (std::vector<float>)detectHair:(std::vector<tensorflow::uint8>)imageData {
    tensorflow::SessionOptions options;
    
    tensorflow::Session* session_pointer = nullptr;
    tensorflow::Status session_status = tensorflow::NewSession(options, &session_pointer);
    if (!session_status.ok()) {
        std::string status_string = session_status.ToString();
        return std::vector<float>();
    }
    std::unique_ptr<tensorflow::Session> session(session_pointer);
    LOG(INFO) << "Session created.";
    
    tensorflow::GraphDef tensorflow_graph;
    LOG(INFO) << "Graph created.";
    
    NSString* network_path = FilePathForResourceName(@"xxx", @"pb");
    PortableReadFileToProto([network_path UTF8String], &tensorflow_graph);
    
    LOG(INFO) << "Creating session.";
    tensorflow::Status s = session->Create(tensorflow_graph);
    if (!s.ok()) {
        LOG(ERROR) << "Could not create TensorFlow Graph: " << s;
        return std::vector<float>();
    }
    
    // Read the image.
    NSString* image_path = FilePathForResourceName(@"2", @"jpg");
    int image_width;
    int image_height;
    int image_channels;
    std::vector<tensorflow::uint8> image_data = LoadImageFromFile(
                                                                  [image_path UTF8String], &image_width, &image_height, &image_channels);
    const int wanted_width = 480;
    const int wanted_height = 480;
    const int wanted_channels = 3;
    assert(image_channels >= wanted_channels);
    tensorflow::Tensor image_tensor(
                                    tensorflow::DT_FLOAT,
                                    tensorflow::TensorShape({
        1, wanted_height, wanted_width, wanted_channels}));
    auto image_tensor_mapped = image_tensor.tensor<float, 4>();
    tensorflow::uint8* in = image_data.data();
    // tensorflow::uint8* in_end = (in + (image_height * image_width * image_channels));
    float* out = image_tensor_mapped.data();
    for (int y = 0; y < wanted_height; ++y) {
        const int in_y = (y * image_height) / wanted_height;
        tensorflow::uint8* in_row = in + (in_y * image_width * image_channels);
        float* out_row = out + (y * wanted_width * wanted_channels);
        for (int x = 0; x < wanted_width; ++x) {
            const int in_x = (x * image_width) / wanted_width;
            tensorflow::uint8* in_pixel = in_row + (in_x * image_channels);
            float* out_pixel = out_row + (x * wanted_channels);
            for (int c = 0; c < wanted_channels; ++c) {
                out_pixel[c] = in_pixel[c];
            }
        }
    }
    
    NSString* result = [network_path stringByAppendingString: @" - loaded!"];
    
    std::string input_layer = "data";
    std::string output_layer = "prob";
    std::vector<tensorflow::Tensor> outputs;
    tensorflow::Status run_status = session->Run({{input_layer, image_tensor}},
                                                 {output_layer}, {}, &outputs);
    if (!run_status.ok()) {
        LOG(ERROR) << "Running model failed: " << run_status;
        tensorflow::LogAllRegisteredKernels();
        result = @"Error running model";
        return std::vector<float>();
    }
    tensorflow::string status_string = run_status.ToString();
    result = [NSString stringWithFormat: @"%@ - %s", result,
              status_string.c_str()];
    
    tensorflow::Tensor* output = &outputs[0];
    std::vector<float> res(480*480*2);
    LOG(INFO) << "Output tensor shape:" <<output->shape();
    LOG(INFO) << "Output tensor data:" <<(*output).tensor<float, (4)>();
//    for(int i=0; i<output->NumElements(); i++) {
//        LOG(INFO) << "Output tensor data:" <<i<<" "<<output->vec<float>()(i);
//        res.push_back(output->vec<float>()(i));
//    }
    
    return res;
}

//辅助函数
namespace {
    class IfstreamInputStream : public ::google::protobuf::io::CopyingInputStream {
    public:
        explicit IfstreamInputStream(const std::string& file_name)
        : ifs_(file_name.c_str(), std::ios::in | std::ios::binary) {}
        ~IfstreamInputStream() { ifs_.close(); }
        
        int Read(void* buffer, int size) {
            if (!ifs_) {
                return -1;
            }
            ifs_.read(static_cast<char*>(buffer), size);
            return (int)ifs_.gcount();
        }
        
    private:
        std::ifstream ifs_;
    };
}  // namespace

bool PortableReadFileToProto(const std::string& file_name,
                             ::google::protobuf::MessageLite* proto) {
    ::google::protobuf::io::CopyingInputStreamAdaptor stream(
                                                             new IfstreamInputStream(file_name));
    stream.SetOwnsCopyingStream(true);
    // TODO(jiayq): the following coded stream is for debugging purposes to allow
    // one to parse arbitrarily large messages for MessageLite. One most likely
    // doesn't want to put protobufs larger than 64MB on Android, so we should
    // eventually remove this and quit loud when a large protobuf is passed in.
    ::google::protobuf::io::CodedInputStream coded_stream(&stream);
    // Total bytes hard limit / warning limit are set to 1GB and 512MB
    // respectively.
    coded_stream.SetTotalBytesLimit(1024LL << 20, 512LL << 20);
    return proto->ParseFromCodedStream(&coded_stream);
}

NSString* FilePathForResourceName(NSString* name, NSString* extension) {
    NSString* file_path = [[NSBundle mainBundle] pathForResource:name ofType:extension];
    if (file_path == NULL) {
        LOG(FATAL) << "Couldn't find '" << [name UTF8String] << "."
        << [extension UTF8String] << "' in bundle.";
    }
    return file_path;
}
```


出现如下错误：
```
Could not create TensorFlow Graph: Invalid argument: No OpKernel was registered to support Op 'Range' with these attrs.  Registered devices: [CPU], Registered kernels:
  device='CPU'; Tidx in [DT_FLOAT]
  device='CPU'; Tidx in [DT_INT32]


[[Node: bottle5_1_up/range_1 = Range[Tidx=DT_INT64](bottle5_1_up/range_1/start, bottle5_1_up/range_1/limit, bottle5_1_up/range_1/delta)]]
```

这个是因为使用的op在库里中没有，但是解决的过程中很奇怪，开始时使用selective op方法，但是还是提示不支持op;最后使用修改makefile文件，将所有op都加入才通过。

## selective op
使用下面的工具生成ops，然后将ops_to_register.h加入到tensorflow源文件中，但是编译过后，还是提示上面相似的错误，原因未知

```
bazel build tensorflow/python/tools:print_selective_registration_header 
bazel-bin/tensorflow/python/tools/print_selective_registration_header \
    --graphs=path/to/graph.pb > ops_to_register.h

```

## 全部编译
在makefile中找到下面的片断，将```ANDROID_TYPES``` 设置为```D__ANDROID_TYPES_FULL__```。
```
# If ANDROID_TYPES is not set assume __ANDROID_TYPES_SLIM__
# ifeq ($(ANDROID_TYPES),)
# 	ANDROID_TYPES := -D__ANDROID_TYPES_SLIM__
# endif
ANDROID_TYPES := -D__ANDROID_TYPES_FULL__
```
在tf_op_files.txt中加入所少op的实现文件，以下以SparseTensorDenseAdd操作符为例：
```
```
tensorflow/core/kernels/sparse_tensor_dense_add_op.cc
```

然后，编译，终于在iPhone运行起来了，但是速度太慢了，估计与自己实现的两个op有关，一个使用for循环遍历了整个数据。

本来想使用tensorflow lite或core ml这两个支持GPU的框架，但是在模型转换过程中都遇到了问题，在[下篇](https://masstone-phoenix.github.io/2018/model_convert_to_coreml.html)文章中讨论记录一下。


<!-- markdown end -->
</div>
<div class="entry" id="main">
<!-- content -->
<p>学习这个的目的是因为想使用机器学习的模型在手机（iOS，android）上使用一个模型进行预测，而且想要使模型尽可能的快，最好能做到实时，这样的话用上GPU就是必需的了，CPU基本上很难做到实时要求的，虽然这不是强需要，但是能满足的话为什么不呢？这样的话经过我们的调研，tensorflow可以通过tfcoreml工具直接转到苹果的Core ML上，而这个框架是支持GPU的；在android上，tensorflow通过调用的The Android Neural Networks API (NNAPI)的方式来支持GPU。</p>

<p>但是，经过我们的实际使用，在iOS这边，tensorflow mobile的速度不容乐观。这里只是记录一下如何让其在iOS上跑起来的，里面的坑还是挺多的。</p>

<h1 id="">搭建模型</h1>

<h1 id="ios">部署到iOS</h1>

<p>先按照<a href="https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/ios#building-the-tensorflow-ios-libraries-from-source">官方文档</a>编译tensorflow库，加入到Xcode工程中，调用如下：</p>

<pre><code>+ (std::vector&lt;float&gt;)detectHair:(std::vector&lt;tensorflow::uint8&gt;)imageData {
    tensorflow::SessionOptions options;

    tensorflow::Session* session_pointer = nullptr;
    tensorflow::Status session_status = tensorflow::NewSession(options, &amp;session_pointer);
    if (!session_status.ok()) {
        std::string status_string = session_status.ToString();
        return std::vector&lt;float&gt;();
    }
    std::unique_ptr&lt;tensorflow::Session&gt; session(session_pointer);
    LOG(INFO) &lt;&lt; "Session created.";

    tensorflow::GraphDef tensorflow_graph;
    LOG(INFO) &lt;&lt; "Graph created.";

    NSString* network_path = FilePathForResourceName(@"xxx", @"pb");
    PortableReadFileToProto([network_path UTF8String], &amp;tensorflow_graph);

    LOG(INFO) &lt;&lt; "Creating session.";
    tensorflow::Status s = session-&gt;Create(tensorflow_graph);
    if (!s.ok()) {
        LOG(ERROR) &lt;&lt; "Could not create TensorFlow Graph: " &lt;&lt; s;
        return std::vector&lt;float&gt;();
    }

    // Read the image.
    NSString* image_path = FilePathForResourceName(@"2", @"jpg");
    int image_width;
    int image_height;
    int image_channels;
    std::vector&lt;tensorflow::uint8&gt; image_data = LoadImageFromFile(
                                                                  [image_path UTF8String], &amp;image_width, &amp;image_height, &amp;image_channels);
    const int wanted_width = 480;
    const int wanted_height = 480;
    const int wanted_channels = 3;
    assert(image_channels &gt;= wanted_channels);
    tensorflow::Tensor image_tensor(
                                    tensorflow::DT_FLOAT,
                                    tensorflow::TensorShape({
        1, wanted_height, wanted_width, wanted_channels}));
    auto image_tensor_mapped = image_tensor.tensor&lt;float, 4&gt;();
    tensorflow::uint8* in = image_data.data();
    // tensorflow::uint8* in_end = (in + (image_height * image_width * image_channels));
    float* out = image_tensor_mapped.data();
    for (int y = 0; y &lt; wanted_height; ++y) {
        const int in_y = (y * image_height) / wanted_height;
        tensorflow::uint8* in_row = in + (in_y * image_width * image_channels);
        float* out_row = out + (y * wanted_width * wanted_channels);
        for (int x = 0; x &lt; wanted_width; ++x) {
            const int in_x = (x * image_width) / wanted_width;
            tensorflow::uint8* in_pixel = in_row + (in_x * image_channels);
            float* out_pixel = out_row + (x * wanted_channels);
            for (int c = 0; c &lt; wanted_channels; ++c) {
                out_pixel[c] = in_pixel[c];
            }
        }
    }

    NSString* result = [network_path stringByAppendingString: @" - loaded!"];

    std::string input_layer = "data";
    std::string output_layer = "prob";
    std::vector&lt;tensorflow::Tensor&gt; outputs;
    tensorflow::Status run_status = session-&gt;Run({{input_layer, image_tensor}},
                                                 {output_layer}, {}, &amp;outputs);
    if (!run_status.ok()) {
        LOG(ERROR) &lt;&lt; "Running model failed: " &lt;&lt; run_status;
        tensorflow::LogAllRegisteredKernels();
        result = @"Error running model";
        return std::vector&lt;float&gt;();
    }
    tensorflow::string status_string = run_status.ToString();
    result = [NSString stringWithFormat: @"%@ - %s", result,
              status_string.c_str()];

    tensorflow::Tensor* output = &amp;outputs[0];
    std::vector&lt;float&gt; res(480*480*2);
    LOG(INFO) &lt;&lt; "Output tensor shape:" &lt;&lt;output-&gt;shape();
    LOG(INFO) &lt;&lt; "Output tensor data:" &lt;&lt;(*output).tensor&lt;float, (4)&gt;();
//    for(int i=0; i&lt;output-&gt;NumElements(); i++) {
//        LOG(INFO) &lt;&lt; "Output tensor data:" &lt;&lt;i&lt;&lt;" "&lt;&lt;output-&gt;vec&lt;float&gt;()(i);
//        res.push_back(output-&gt;vec&lt;float&gt;()(i));
//    }

    return res;
}

//辅助函数
namespace {
    class IfstreamInputStream : public ::google::protobuf::io::CopyingInputStream {
    public:
        explicit IfstreamInputStream(const std::string&amp; file_name)
        : ifs_(file_name.c_str(), std::ios::in | std::ios::binary) {}
        ~IfstreamInputStream() { ifs_.close(); }

        int Read(void* buffer, int size) {
            if (!ifs_) {
                return -1;
            }
            ifs_.read(static_cast&lt;char*&gt;(buffer), size);
            return (int)ifs_.gcount();
        }

    private:
        std::ifstream ifs_;
    };
}  // namespace

bool PortableReadFileToProto(const std::string&amp; file_name,
                             ::google::protobuf::MessageLite* proto) {
    ::google::protobuf::io::CopyingInputStreamAdaptor stream(
                                                             new IfstreamInputStream(file_name));
    stream.SetOwnsCopyingStream(true);
    // TODO(jiayq): the following coded stream is for debugging purposes to allow
    // one to parse arbitrarily large messages for MessageLite. One most likely
    // doesn't want to put protobufs larger than 64MB on Android, so we should
    // eventually remove this and quit loud when a large protobuf is passed in.
    ::google::protobuf::io::CodedInputStream coded_stream(&amp;stream);
    // Total bytes hard limit / warning limit are set to 1GB and 512MB
    // respectively.
    coded_stream.SetTotalBytesLimit(1024LL &lt;&lt; 20, 512LL &lt;&lt; 20);
    return proto-&gt;ParseFromCodedStream(&amp;coded_stream);
}

NSString* FilePathForResourceName(NSString* name, NSString* extension) {
    NSString* file_path = [[NSBundle mainBundle] pathForResource:name ofType:extension];
    if (file_path == NULL) {
        LOG(FATAL) &lt;&lt; "Couldn't find '" &lt;&lt; [name UTF8String] &lt;&lt; "."
        &lt;&lt; [extension UTF8String] &lt;&lt; "' in bundle.";
    }
    return file_path;
}
</code></pre>

<p>出现如下错误：</p>

<pre><code>Could not create TensorFlow Graph: Invalid argument: No OpKernel was registered to support Op 'Range' with these attrs.&nbsp; Registered devices: [CPU], Registered kernels:
&nbsp; device='CPU'; Tidx in [DT_FLOAT]
&nbsp; device='CPU'; Tidx in [DT_INT32]


[[Node: bottle5_1_up/range_1 = Range[Tidx=DT_INT64](bottle5_1_up/range_1/start, bottle5_1_up/range_1/limit, bottle5_1_up/range_1/delta)]]
</code></pre>

<p>这个是因为使用的op在库里中没有，但是解决的过程中很奇怪，开始时使用selective op方法，但是还是提示不支持op;最后使用修改makefile文件，将所有op都加入才通过。</p>

<h2 id="selectiveop">selective op</h2>

<p>使用下面的工具生成ops，然后将ops<em>to</em>register.h加入到tensorflow源文件中，但是编译过后，还是提示上面相似的错误，原因未知</p>

<pre><code>bazel build tensorflow/python/tools:print_selective_registration_header 
bazel-bin/tensorflow/python/tools/print_selective_registration_header \
    --graphs=path/to/graph.pb &gt; ops_to_register.h
</code></pre>

<h2 id="">全部编译</h2>

<p>在makefile中找到下面的片断，将<code>ANDROID_TYPES</code> 设置为<code>D__ANDROID_TYPES_FULL__</code>。</p>

<pre><code># If ANDROID_TYPES is not set assume __ANDROID_TYPES_SLIM__
# ifeq ($(ANDROID_TYPES),)
#     ANDROID_TYPES := -D__ANDROID_TYPES_SLIM__
# endif
ANDROID_TYPES := -D__ANDROID_TYPES_FULL__
</code></pre>

<p>在tf<em>op</em>files.txt中加入所少op的实现文件，以下以SparseTensorDenseAdd操作符为例：</p>

<pre><code>```
tensorflow/core/kernels/sparse_tensor_dense_add_op.cc
</code></pre>

<p>然后，编译，终于在iPhone运行起来了，但是速度太慢了，估计与自己实现的两个op有关，一个使用for循环遍历了整个数据。</p>

<p>本来想使用tensorflow lite或core ml这两个支持GPU的框架，但是在模型转换过程中都遇到了问题，在<a href="https://masstone-phoenix.github.io/2018/model_convert_to_coreml.html">下篇</a>文章中讨论记录一下。</p>
<!-- content end -->
</div>
<br>
<br>
    <div id="disqus_thread"></div>
	<div class="footer">
		<p>© Copyright 2014 by isnowfy, Designed by isnowfy</p>
	</div>
</div>
<script src="main.js"></script>
<script id="content" type="text/mustache">
    <h1>{{title}}</h1>
    <div class="tag">
    {{date}}
    {{#tags}}
    <a href="/#/tag/{{name}}">#{{name}}</a>
    {{/tags}}
    </div>
</script>
<script id="pagesTemplate" type="text/mustache">
    {{#pages}}
    <li>
        <a href="{{path}}">{{title}}</a>
    </li>
    {{/pages}}
</script>
<script>
$(document).ready(function() {
    $.ajax({
        url: "main.json",
        type: "GET",
        dataType: "json",
        success: function(data) {
            $("#title").html(data.name);
            var pagesTemplate = Hogan.compile($("#pagesTemplate").html());
            var pagesHtml = pagesTemplate.render({"pages": data.pages});
            $("#pages").append(pagesHtml);
            //path
            var path = "2018/tensorflow_using_in_iOS.html";
            //path end
            var now = 0;
            for (var i = 0; i < data.posts.length; ++i)
                if (path == data.posts[i].path)
                    now = i;
            var post = data.posts[now];
            var tmp = post.tags.split(" ");
            var tags = [];
            for (var i = 0; i < tmp.length; ++i)
                if (tmp[i].length > 0)
                    tags.push({"name": tmp[i]});
            var contentTemplate = Hogan.compile($("#content").html());
            var contentHtml = contentTemplate.render({"title": post.title, "tags": tags, "date": post.date});
            $("#main").prepend(contentHtml);
            if (data.disqus_shortname.length > 0) {
                var disqus_shortname = data.disqus_shortname;
                (function() {
                    var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
                    dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
                    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
                })();
            }
        }
    });
});
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ["\\(", "\\)"]], processEscapes: true}});
</script>
</body>
</html>
