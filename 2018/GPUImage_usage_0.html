<!DOCTYPE HTML>
<html>
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
    <meta name="Keywords" content="blog"/>
    <meta name="Description" content="blog"/>
    <title>Simple</title>
    <link rel="shortcut icon" href="/static/favicon.png"/>
    <link rel="stylesheet" type="text/css" href="/main.css" />
</head>
<body>
<div class="main">
    <div class="header">
    	<ul id="pages">
            <li><a href="/">home</a></li>
            <li><a href="/#/tags">tags</a></li>
            <li><a href="/#/archive">archive</a></li>
    	</ul>
    </div>
	<div class="wrap-header">
	<h1>
    <a href="/" id="title"></a>
	</h1>
	</div>
<div id="md" style="display: none;">
<!-- markdown -->
当在iOS中对图片应用滤镜时，一般有两个选择一个是Core Image， 一个是GPUImage。
我这两天写了一个UIImage的Category，用于对图片应用一些类型的滤镜。本文总结了一下这个过程。大概分为三个阶段：

1. 比较Core Image的GPUImage
1. GPUImage的一些基本知识，其中会对GPUImage的一些基础类进行分析
1. GPUImage的使用——其实这部分可有可无，网上大多都是这个，在这里只是简单写一下，为了以后忘记时可以随时查阅。

# Core Image VS GPUImage
对一个图片应用滤镜或其它算法（比如美白），直接想的到方法是使用OpenGL ES，但是直接上这个感觉“杀鸡用牛刀”，还有一个是其它不了解OpenGL的人没办法帮你review。于是就想到了iOS上现成的两个封装Core Image 和 GPUImage。以下对比一下它们各自的优势，共同点就不在描述了：

## Core Image
- 官方框架，与Core Graphics，Core Animation等无缝结合
- 能够根据设备不同选择最优的渲染器，依次为 Metal，OpenGL ES，Core Graphics。
- 能够支持CPU渲染，其实也在就程序在进入后台时还能够处理图片
- 滤镜链的性能比 GPUImage 高(没有验证过，[这里](https://stackoverflow.com/questions/24153103/key-differences-between-core-image-and-gpuimage/24173342#24173342)是这么说的)
- 支持对大图进行处理，超过 GPU 纹理限制 (4096 * 4096)的时候，会自动拆分成几个小块处理(Automatic tiling)。GPUImage 当处理超过纹理限制的图像时候，会先做判断，压缩成最大纹理限制的图像，导致图像质量损失。


## GPUImage
- 作者说他最初的想法只是实现一个对OpenGL ES的封装，现在主攻方向是机器视觉
- 支持的iOS版本比Core Image更多（也就是支持一些低版本）
- 定制更加复杂的管线操作，可定制程度高
- 最大的优势（我认为的）就是它是开源的

其实如果光是考虑iOS，我还是选择Core Image，因为它的性能与GPUImage目前看是不相上下，也可以自定义滤镜算法，还是官方框架，目前我们的程序也不需要支持低版本的iOS，但是，我们的项目中有Android，也有iOS，这是我们选择GPUImage的一个主要原因——shader程序可以复用。

# GPUImage的一些基本知识
GPUImage对图形数据的处理流程是这样的：Source -> filter1 -> filter2 .... -> Output。可以将它理解为一个链。

## GLProgram
GLProgram是对OpenGL中program和shader的封装，用户只需要提供vertex shader和fragment shader源代码的字符串就可以创建一个绘图时使用的program。同时提供了link，use等接口。

## GPUImageContext
GPUImageContext提供了对OpenGL中context的封装，由于这个OpenGL的context不是线程安全的，GPUImageContext同时还将所有的绘制操作都移动到一个自定义的队列中；同时，它提供了对于GLProgram和GPUImageFramebuffer的一些缓存机制。

## GPUImageFramebuffer
这个封装了OpenGL中的framebuffer，同时提供了refernce count的管理，方便缓存的使用。

值得注意的是，从GPU读取Image数据到CPU时，iOS有一个[texture cache的概念](http://allmybrain.com/2011/12/08/rendering-to-a-texture-with-ios-5-texture-cache-api/)，通过它比glReadPixels快速很多。

具体如下：
```
//先定义一个创建CVPixelBuffer的empty IOSurface properties dictionary
CFDictionaryRef empty; // empty value for attr value.
CFMutableDictionaryRef attrs;
empty = CFDictionaryCreate(kCFAllocatorDefault, NULL, NULL, 0, &kCFTypeDictionaryKeyCallBacks, &kCFTypeDictionaryValueCallBacks); // our empty IOSurface properties dictionary
attrs = CFDictionaryCreateMutable(kCFAllocatorDefault, 1, &kCFTypeDictionaryKeyCallBacks, &kCFTypeDictionaryValueCallBacks);
CFDictionarySetValue(attrs, kCVPixelBufferIOSurfacePropertiesKey, empty);

//创建CVPixelBuffer，存在renderTarget变量中
CVReturn err = CVPixelBufferCreate(kCFAllocatorDefault, (int)_size.width, (int)_size.height, kCVPixelFormatType_32BGRA, attrs, &renderTarget);
if (err)
{
    NSLog(@"FBO size: %f, %f", _size.width, _size.height);
    NSAssert(NO, @"Error at CVPixelBufferCreate %d", err);
}

//利用 CVOpenGLESTextureCache从我们上面的renderTarget创建一个texture
err = CVOpenGLESTextureCacheCreateTextureFromImage (kCFAllocatorDefault, coreVideoTextureCache, renderTarget,
                                                    NULL, // texture attributes
                                                    GL_TEXTURE_2D,
                                                    _textureOptions.internalFormat, // opengl format
                                                    (int)_size.width,
                                                    (int)_size.height,
                                                    _textureOptions.format, // native iOS format
                                                    _textureOptions.type,
                                                    0,
                                                    &renderTexture);

CFRelease(attrs);
CFRelease(empty);
//绑定texture并设置相应属性
glBindTexture(CVOpenGLESTextureGetTarget(renderTexture), CVOpenGLESTextureGetName(renderTexture));
_texture = CVOpenGLESTextureGetName(renderTexture);
glTexParameterf(GL_TEXTURE_2D, GL_TEXTURE_WRAP_S, _textureOptions.wrapS);
glTexParameterf(GL_TEXTURE_2D, GL_TEXTURE_WRAP_T, _textureOptions.wrapT);

//将texture绑定到要绘制的framebuffer上
glFramebufferTexture2D(GL_FRAMEBUFFER, GL_COLOR_ATTACHMENT0, GL_TEXTURE_2D, CVOpenGLESTextureGetName(renderTexture), 0);

//这里你就可以绘制你想绘制的任何事情了


//读取数据时使用下面的方法
if (kCVReturnSuccess == CVPixelBufferLockBaseAddress(renderTarget,
                                                     kCVPixelBufferLock_ReadOnly)) {
    uint8_t* pixels=(uint8_t*)CVPixelBufferGetBaseAddress(renderTarget);
    // process pixels how you like!
    CVPixelBufferUnlockBaseAddress(m_rgb, kCVPixelBufferLock_ReadOnly);
  }

```



```
//这里补充一下，上面用到的coreVideoTextureCache是在GPUImageContext中使用如下函数创建的，它的作用是维护管理它所创建的CVOpenGLESTextureCache：

CVReturn err = CVOpenGLESTextureCacheCreate(kCFAllocatorDefault, NULL, (__bridge void *)[self context], NULL, &_coreVideoTextureCache);
```

## GPUImageInput
这个协议定义了如何为链条中的一个节点是如何接收输入数据，以及如何推动数据向下一级节点流动的函数。除此之外，还有一个接口nextAvailableTextureIndex来获取当前节点的下一个可用texture的索引。

```
- (void)setInputFramebuffer:(GPUImageFramebuffer *)newInputFramebuffer atIndex:(NSInteger)textureIndex;
```
函数setInputFramebuffer:atIndex:为当前节点设置输入的framebuffer，其中textureIndex是指当前节点的第几个texture使用输入图像的framebuffer，即OpenGL中fragment shader中的使用的texture的索引值。

```
- (void)newFrameReadyAtTime:(CMTime)frameTime atIndex:(NSInteger)textureIndex;
{
    static const GLfloat imageVertices[] = {
        -1.0f, -1.0f,
        1.0f, -1.0f,
        -1.0f,  1.0f,
        1.0f,  1.0f,
    };
    
    [self renderToTextureWithVertices:imageVertices textureCoordinates:[[self class] textureCoordinatesForRotation:inputRotation]];

    [self informTargetsAboutNewFrameAtTime:frameTime];
}
```
函数newFrameReadyAtTime:atIndex:是当前节点处理完图像后，推动数据流向下一级处理的函数。其中的textureIndex是在调用addTarget:函数时计算并存储起来的，就是下一级节点的索引。它的工作很简单，先调用renderToTextureWithVertices:textureCoordinates:渲染图像，然后调用informTargetsAboutNewFrameAtTime:通知输出加入的所有targets。

## GPUImageOutput
GPUImageOutput是所有输出的超类，提供了管理targets的一些方法，以及输入图像比如newCGImageFromCurrentlyProcessedOutput的接口，具体的实现在子类中可以自定义。

## GPUImageFilter
这是所有filter的超类，同时是GPUImageOutput的子类，其中我们可以学习到自定义滤镜的一些操作，同时，它实现了newCGImageFromCurrentlyProcessedOutput等接口，可以直接进行使用。

# GPUImage的使用
## 滤镜已经有现成的

```
    UIImage *inputImage = self;
    UIImage *outputImage = nil;
    GPUImagePicture *stillImageSource = [[GPUImagePicture alloc] initWithImage:inputImage];
    //添加滤镜
    GPUImageLookupFilter *lookUpFilter = [[GPUImageLookupFilter alloc] init];
    //导入之前保存的NewLookupTable.png文件
    GPUImagePicture *lookupImg = [[GPUImagePicture alloc] initWithImage:lookUpTableImage];
    [lookupImg addTarget:lookUpFilter atTextureLocation:1];
    [stillImageSource addTarget:lookUpFilter atTextureLocation:0];
    [lookUpFilter useNextFrameForImageCapture];
    if([lookupImg processImageWithCompletionHandler:nil] && [stillImageSource processImageWithCompletionHandler:nil]) {
        outputImage= [lookUpFilter imageFromCurrentFramebuffer];
    }
    return outputImage;


```

先是将输入与滤镜连接起来，然后调用processImageWithCompletionHandler进行处理，最后使用imageFromCurrentFramebuffer读取处理后的结果。

## 自定义滤镜
这里我们以一个简单的美白滤镜的源代码为例，最主要的是写一个fragment shader来进行美白；其次，将这个shader的源代码字符串输入；最后，定义修改算法内参数的接口。如下：

```
NSString *const kGPUImageColorMatrixFragmentShaderString = SHADER_STRING
(
 varying highp vec2 textureCoordinate;
 
 uniform sampler2D inputImageTexture;
 
 uniform lowp float beta;
 
 void main()
 {
     lowp vec4 textureColor = texture2D(inputImageTexture, textureCoordinate);
     gl_FragColor = log(textureColor * (beta - 1.0) + 1) / log(beta);
 }
 );


@implementation GPUImageWhiteningFilter
#pragma mark -
#pragma mark Initialization and teardown

- (id)init;
{
    if (!(self = [super initWithFragmentShaderFromString:kGPUImageColorMatrixFragmentShaderString]))
    {
        return nil;
    }
    
    betaUniform = [filterProgram uniformIndex:@"beta"];
    
    self.beta = 2.f;
    return self;
}

#pragma mark -
#pragma mark Accessors
- (void)setBeta:(CGFloat)aBeta {
    if (aBeta < 1.0) {
        return;
    }
    _beta = aBeta;
    [self setFloat:aBeta forUniform:betaUniform program:filterProgram];
    
}
@end

```

使用这个滤镜时，与上面使用现有滤镜大同小异。
<!-- markdown end -->
</div>
<div class="entry" id="main">
<!-- content -->
<p>当在iOS中对图片应用滤镜时，一般有两个选择一个是Core Image， 一个是GPUImage。
我这两天写了一个UIImage的Category，用于对图片应用一些类型的滤镜。本文总结了一下这个过程。大概分为三个阶段：</p>

<ol>
<li>比较Core Image的GPUImage</li>
<li>GPUImage的一些基本知识，其中会对GPUImage的一些基础类进行分析</li>
<li>GPUImage的使用——其实这部分可有可无，网上大多都是这个，在这里只是简单写一下，为了以后忘记时可以随时查阅。</li>
</ol>

<h1 id="coreimagevsgpuimage">Core Image VS GPUImage</h1>

<p>对一个图片应用滤镜或其它算法（比如美白），直接想的到方法是使用OpenGL ES，但是直接上这个感觉“杀鸡用牛刀”，还有一个是其它不了解OpenGL的人没办法帮你review。于是就想到了iOS上现成的两个封装Core Image 和 GPUImage。以下对比一下它们各自的优势，共同点就不在描述了：</p>

<h2 id="coreimage">Core Image</h2>

<ul>
<li>官方框架，与Core Graphics，Core Animation等无缝结合</li>
<li>能够根据设备不同选择最优的渲染器，依次为 Metal，OpenGL ES，Core Graphics。</li>
<li>能够支持CPU渲染，其实也在就程序在进入后台时还能够处理图片</li>
<li>滤镜链的性能比 GPUImage 高(没有验证过，<a href="https://stackoverflow.com/questions/24153103/key-differences-between-core-image-and-gpuimage/24173342#24173342">这里</a>是这么说的)</li>
<li>支持对大图进行处理，超过 GPU 纹理限制 (4096 * 4096)的时候，会自动拆分成几个小块处理(Automatic tiling)。GPUImage 当处理超过纹理限制的图像时候，会先做判断，压缩成最大纹理限制的图像，导致图像质量损失。</li>
</ul>

<h2 id="gpuimage">GPUImage</h2>

<ul>
<li>作者说他最初的想法只是实现一个对OpenGL ES的封装，现在主攻方向是机器视觉</li>
<li>支持的iOS版本比Core Image更多（也就是支持一些低版本）</li>
<li>定制更加复杂的管线操作，可定制程度高</li>
<li>最大的优势（我认为的）就是它是开源的</li>
</ul>

<p>其实如果光是考虑iOS，我还是选择Core Image，因为它的性能与GPUImage目前看是不相上下，也可以自定义滤镜算法，还是官方框架，目前我们的程序也不需要支持低版本的iOS，但是，我们的项目中有Android，也有iOS，这是我们选择GPUImage的一个主要原因——shader程序可以复用。</p>

<h1 id="gpuimage">GPUImage的一些基本知识</h1>

<p>GPUImage对图形数据的处理流程是这样的：Source -&gt; filter1 -&gt; filter2 .... -&gt; Output。可以将它理解为一个链。</p>

<h2 id="glprogram">GLProgram</h2>

<p>GLProgram是对OpenGL中program和shader的封装，用户只需要提供vertex shader和fragment shader源代码的字符串就可以创建一个绘图时使用的program。同时提供了link，use等接口。</p>

<h2 id="gpuimagecontext">GPUImageContext</h2>

<p>GPUImageContext提供了对OpenGL中context的封装，由于这个OpenGL的context不是线程安全的，GPUImageContext同时还将所有的绘制操作都移动到一个自定义的队列中；同时，它提供了对于GLProgram和GPUImageFramebuffer的一些缓存机制。</p>

<h2 id="gpuimageframebuffer">GPUImageFramebuffer</h2>

<p>这个封装了OpenGL中的framebuffer，同时提供了refernce count的管理，方便缓存的使用。</p>

<p>值得注意的是，从GPU读取Image数据到CPU时，iOS有一个<a href="http://allmybrain.com/2011/12/08/rendering-to-a-texture-with-ios-5-texture-cache-api/">texture cache的概念</a>，通过它比glReadPixels快速很多。</p>

<p>具体如下：</p>

<pre><code>//先定义一个创建CVPixelBuffer的empty IOSurface properties dictionary
CFDictionaryRef empty; // empty value for attr value.
CFMutableDictionaryRef attrs;
empty = CFDictionaryCreate(kCFAllocatorDefault, NULL, NULL, 0, &amp;kCFTypeDictionaryKeyCallBacks, &amp;kCFTypeDictionaryValueCallBacks); // our empty IOSurface properties dictionary
attrs = CFDictionaryCreateMutable(kCFAllocatorDefault, 1, &amp;kCFTypeDictionaryKeyCallBacks, &amp;kCFTypeDictionaryValueCallBacks);
CFDictionarySetValue(attrs, kCVPixelBufferIOSurfacePropertiesKey, empty);

//创建CVPixelBuffer，存在renderTarget变量中
CVReturn err = CVPixelBufferCreate(kCFAllocatorDefault, (int)_size.width, (int)_size.height, kCVPixelFormatType_32BGRA, attrs, &amp;renderTarget);
if (err)
{
    NSLog(@"FBO size: %f, %f", _size.width, _size.height);
    NSAssert(NO, @"Error at CVPixelBufferCreate %d", err);
}

//利用 CVOpenGLESTextureCache从我们上面的renderTarget创建一个texture
err = CVOpenGLESTextureCacheCreateTextureFromImage (kCFAllocatorDefault, coreVideoTextureCache, renderTarget,
                                                    NULL, // texture attributes
                                                    GL_TEXTURE_2D,
                                                    _textureOptions.internalFormat, // opengl format
                                                    (int)_size.width,
                                                    (int)_size.height,
                                                    _textureOptions.format, // native iOS format
                                                    _textureOptions.type,
                                                    0,
                                                    &amp;renderTexture);

CFRelease(attrs);
CFRelease(empty);
//绑定texture并设置相应属性
glBindTexture(CVOpenGLESTextureGetTarget(renderTexture), CVOpenGLESTextureGetName(renderTexture));
_texture = CVOpenGLESTextureGetName(renderTexture);
glTexParameterf(GL_TEXTURE_2D, GL_TEXTURE_WRAP_S, _textureOptions.wrapS);
glTexParameterf(GL_TEXTURE_2D, GL_TEXTURE_WRAP_T, _textureOptions.wrapT);

//将texture绑定到要绘制的framebuffer上
glFramebufferTexture2D(GL_FRAMEBUFFER, GL_COLOR_ATTACHMENT0, GL_TEXTURE_2D, CVOpenGLESTextureGetName(renderTexture), 0);

//这里你就可以绘制你想绘制的任何事情了


//读取数据时使用下面的方法
if (kCVReturnSuccess == CVPixelBufferLockBaseAddress(renderTarget,
                                                     kCVPixelBufferLock_ReadOnly)) {
    uint8_t* pixels=(uint8_t*)CVPixelBufferGetBaseAddress(renderTarget);
    // process pixels how you like!
    CVPixelBufferUnlockBaseAddress(m_rgb, kCVPixelBufferLock_ReadOnly);
  }
</code></pre>

<pre><code>//这里补充一下，上面用到的coreVideoTextureCache是在GPUImageContext中使用如下函数创建的，它的作用是维护管理它所创建的CVOpenGLESTextureCache：

CVReturn err = CVOpenGLESTextureCacheCreate(kCFAllocatorDefault, NULL, (__bridge void *)[self context], NULL, &amp;_coreVideoTextureCache);
</code></pre>

<h2 id="gpuimageinput">GPUImageInput</h2>

<p>这个协议定义了如何为链条中的一个节点是如何接收输入数据，以及如何推动数据向下一级节点流动的函数。除此之外，还有一个接口nextAvailableTextureIndex来获取当前节点的下一个可用texture的索引。</p>

<pre><code>- (void)setInputFramebuffer:(GPUImageFramebuffer *)newInputFramebuffer atIndex:(NSInteger)textureIndex;
</code></pre>

<p>函数setInputFramebuffer:atIndex:为当前节点设置输入的framebuffer，其中textureIndex是指当前节点的第几个texture使用输入图像的framebuffer，即OpenGL中fragment shader中的使用的texture的索引值。</p>

<pre><code>- (void)newFrameReadyAtTime:(CMTime)frameTime atIndex:(NSInteger)textureIndex;
{
    static const GLfloat imageVertices[] = {
        -1.0f, -1.0f,
        1.0f, -1.0f,
        -1.0f,  1.0f,
        1.0f,  1.0f,
    };

    [self renderToTextureWithVertices:imageVertices textureCoordinates:[[self class] textureCoordinatesForRotation:inputRotation]];

    [self informTargetsAboutNewFrameAtTime:frameTime];
}
</code></pre>

<p>函数newFrameReadyAtTime:atIndex:是当前节点处理完图像后，推动数据流向下一级处理的函数。其中的textureIndex是在调用addTarget:函数时计算并存储起来的，就是下一级节点的索引。它的工作很简单，先调用renderToTextureWithVertices:textureCoordinates:渲染图像，然后调用informTargetsAboutNewFrameAtTime:通知输出加入的所有targets。</p>

<h2 id="gpuimageoutput">GPUImageOutput</h2>

<p>GPUImageOutput是所有输出的超类，提供了管理targets的一些方法，以及输入图像比如newCGImageFromCurrentlyProcessedOutput的接口，具体的实现在子类中可以自定义。</p>

<h2 id="gpuimagefilter">GPUImageFilter</h2>

<p>这是所有filter的超类，同时是GPUImageOutput的子类，其中我们可以学习到自定义滤镜的一些操作，同时，它实现了newCGImageFromCurrentlyProcessedOutput等接口，可以直接进行使用。</p>

<h1 id="gpuimage">GPUImage的使用</h1>

<h2 id="">滤镜已经有现成的</h2>

<pre><code>    UIImage *inputImage = self;
    UIImage *outputImage = nil;
    GPUImagePicture *stillImageSource = [[GPUImagePicture alloc] initWithImage:inputImage];
    //添加滤镜
    GPUImageLookupFilter *lookUpFilter = [[GPUImageLookupFilter alloc] init];
    //导入之前保存的NewLookupTable.png文件
    GPUImagePicture *lookupImg = [[GPUImagePicture alloc] initWithImage:lookUpTableImage];
    [lookupImg addTarget:lookUpFilter atTextureLocation:1];
    [stillImageSource addTarget:lookUpFilter atTextureLocation:0];
    [lookUpFilter useNextFrameForImageCapture];
    if([lookupImg processImageWithCompletionHandler:nil] &amp;&amp; [stillImageSource processImageWithCompletionHandler:nil]) {
        outputImage= [lookUpFilter imageFromCurrentFramebuffer];
    }
    return outputImage;
</code></pre>

<p>先是将输入与滤镜连接起来，然后调用processImageWithCompletionHandler进行处理，最后使用imageFromCurrentFramebuffer读取处理后的结果。</p>

<h2 id="">自定义滤镜</h2>

<p>这里我们以一个简单的美白滤镜的源代码为例，最主要的是写一个fragment shader来进行美白；其次，将这个shader的源代码字符串输入；最后，定义修改算法内参数的接口。如下：</p>

<pre><code>NSString *const kGPUImageColorMatrixFragmentShaderString = SHADER_STRING
(
 varying highp vec2 textureCoordinate;

 uniform sampler2D inputImageTexture;

 uniform lowp float beta;

 void main()
 {
     lowp vec4 textureColor = texture2D(inputImageTexture, textureCoordinate);
     gl_FragColor = log(textureColor * (beta - 1.0) + 1) / log(beta);
 }
 );


@implementation GPUImageWhiteningFilter
#pragma mark -
#pragma mark Initialization and teardown

- (id)init;
{
    if (!(self = [super initWithFragmentShaderFromString:kGPUImageColorMatrixFragmentShaderString]))
    {
        return nil;
    }

    betaUniform = [filterProgram uniformIndex:@"beta"];

    self.beta = 2.f;
    return self;
}

#pragma mark -
#pragma mark Accessors
- (void)setBeta:(CGFloat)aBeta {
    if (aBeta &lt; 1.0) {
        return;
    }
    _beta = aBeta;
    [self setFloat:aBeta forUniform:betaUniform program:filterProgram];

}
@end
</code></pre>

<p>使用这个滤镜时，与上面使用现有滤镜大同小异。</p>
<!-- content end -->
</div>
<br>
<br>
    <div id="disqus_thread"></div>
	<div class="footer">
		<p>© Copyright 2014 by isnowfy, Designed by isnowfy</p>
	</div>
</div>
<script src="main.js"></script>
<script id="content" type="text/mustache">
    <h1>{{title}}</h1>
    <div class="tag">
    {{date}}
    {{#tags}}
    <a href="/#/tag/{{name}}">#{{name}}</a>
    {{/tags}}
    </div>
</script>
<script id="pagesTemplate" type="text/mustache">
    {{#pages}}
    <li>
        <a href="{{path}}">{{title}}</a>
    </li>
    {{/pages}}
</script>
<script>
$(document).ready(function() {
    $.ajax({
        url: "main.json",
        type: "GET",
        dataType: "json",
        success: function(data) {
            $("#title").html(data.name);
            var pagesTemplate = Hogan.compile($("#pagesTemplate").html());
            var pagesHtml = pagesTemplate.render({"pages": data.pages});
            $("#pages").append(pagesHtml);
            //path
            var path = "2018/GPUImage_usage_0.html";
            //path end
            var now = 0;
            for (var i = 0; i < data.posts.length; ++i)
                if (path == data.posts[i].path)
                    now = i;
            var post = data.posts[now];
            var tmp = post.tags.split(" ");
            var tags = [];
            for (var i = 0; i < tmp.length; ++i)
                if (tmp[i].length > 0)
                    tags.push({"name": tmp[i]});
            var contentTemplate = Hogan.compile($("#content").html());
            var contentHtml = contentTemplate.render({"title": post.title, "tags": tags, "date": post.date});
            $("#main").prepend(contentHtml);
            if (data.disqus_shortname.length > 0) {
                var disqus_shortname = data.disqus_shortname;
                (function() {
                    var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
                    dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
                    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
                })();
            }
        }
    });
});
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ["\\(", "\\)"]], processEscapes: true}});
</script>
</body>
</html>
