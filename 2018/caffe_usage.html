<!DOCTYPE HTML>
<html>
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
    <meta name="Keywords" content="blog"/>
    <meta name="Description" content="blog"/>
    <title>Simple</title>
    <link rel="shortcut icon" href="/static/favicon.png"/>
    <link rel="stylesheet" type="text/css" href="/main.css" />
</head>
<body>
<div class="main">
    <div class="header">
    	<ul id="pages">
            <li><a href="/">home</a></li>
            <li><a href="/#/tags">tags</a></li>
            <li><a href="/#/archive">archive</a></li>
    	</ul>
    </div>
	<div class="wrap-header">
	<h1>
    <a href="/" id="title"></a>
	</h1>
	</div>
<div id="md" style="display: none;">
<!-- markdown -->
项目是用到一个模型是公司的模型，但是只有一个caffe的版本，正好学习一下caffe的使用。

# 简介
这里主要是对caffe作者的一个[教程](http://on-demand.gputechconf.com/gtc/2014/webinar/gtc-express-deep-learning-caffee-evan-shelhamer.pdf)的总结，也参考了[这篇文章](http://www.datakit.cn/blog/2015/06/12/online_meet_up_with_yangqing_jia.html)。

## caffe起源

大家最近一段时间应该已经听到很多关于deep learning的八卦了，deep learning比较流行的一个原因，***主要是因为它能够自主地从数据上学到有用的feature***,特别是对于一些不知道如何设计feature的场合，比如说图像和speech deep learning可以学习到比以往比如说sift或者MFCC这样手工设计的feature更好的方法, 这些feature有很强的semantic的含义。所以很多时候在用到其他的一些任务的时候会很有效，这也是为什么我们可以用一个feature来实现很多比如说识别，检测，物体分割这样的不同任务的缘故。

无论如何，深度学习其实说回来是个挺久的话题了，Yann Lecun在89年的时候就提出了convolutional Neural Net的想法，然后在手写数字上获得了很大的成功。最近深度学习重新受到关注，最大的原因是两个：

- 一个是大规模的数据集使得我们可以学习到远比digit更加复杂的概念
- 另外一个是大规模并行计算让我们可以做很快的优化，使得以前我们没法想象的计算量都变成很容易实现的了

所以这些都很美好。但是问题是写code还挺麻烦的。所以大家肯定希望有个比较好用的框架来很快上手和试试这些深度学习的算法。 所以这就是Caffe了！Caffe是作者在Berkeley写论文的时候想学习C++和cuda写的，然后写完了觉得我自己用太亏了，所以想贡献给社区让大家来用。所以如果你看见一些写得很烂的code，不要骂作者。

## caffe介绍
caffe的好处是，我们基本上可以用一个比较简单的语言（google protobuffer）来定义许多网络结构，然后我们可以在CPU或者GPU上面执行这些代码，而且cpu和gpu在数学结果上是兼容的。然后，所有的模型和recipe我们都会公布出来，使得我们可以很容易地reproduce互相发布的结果。这也是我感到很幸运的一个地方，大家都很喜欢caffe，也很喜欢分享自己paper里的成果（比如说MIT的place net和VGG的模型）。

无论如何，这就是Caffe的简单介绍了，最开始是一个hobby project，但是最近Berkeley和其他公司比如说NVidia，Yahoo在很认真地维护它，希望能够把整个架构做的更好用。

然后我大概讲一下caffe的设计吧。

基本上，caffe 遵循了神经网络的一个简单假设 - 所有的计算都是以layer的形式表示的layer做的事情就是拿到一些数据，然后输出一些计算以后的结果，比如说卷积，就是输入一个图像，然后和这一层的参数（filter）做卷积,然后输出卷积的结果。每一个layer需要做两个计算：forward是从输入计算输出，然后backward是从上面给的gradient来计算相对于输入的gradient。只要这两个函数实现了以后，我们就可以把很多层连接成一个网络，这个网络做的事情就是输入我们的数据（图像或者语音或者whatever），然后来计算我们需要的输出（比如说识别的label）。只要这两个函数实现了以后，我们就可以把很多层连接成一个网络，这个网络做的事情就是输入我们的数据（图像或者语音或者whatever），然后来计算我们需要的输出（比如说识别的label）。在训练的时候，我们可以根据已有的label来计算loss和gradient，然后用gradient来update网络的参数。这个就是Caffe的一个基本流程！

如果大家需要自己实现一个layer的话，可以参考下图。比如说输入是x，我们可以想象一个layer的forward function就是y=f(x) 然后，我们会有一个loss function，记成L(.)。在做backward的时候，网络得到的是上层给出的gradient，dL/dy，然后网络需要做的计算是dL/dx = dL/dy * dy/dx，dy/dx也就是f’(x)。于是，这样我们就可以一层一层往后计算gradient。Caffe里面实现的solver主要也是为了神经网络设计的。

![](https://masstone-phoenix.github.io/image/caffe/caffe_layer.png)

在做training的时候，我们一般都会做SGD，就是每次输入一个小batch，做计算，update参数，然后再输入下一个batch Caffe也实现了许多实际应用上比简单SGD要更有效的算法，比如说momentum 和Adagrad （顺便插一句，Ilya Sutskever有paper解释说，momemtum其实已经可以很好地实现quasi second order的优化，所以建议大家可以从momentum sgd开始尝试做training）.

基本上，最简单地用caffe上手的方法就是：

- Convert the data to a Caffe format
- lmdb, leveldb, hdf5 / .mat, list of images, etc.
- Define the Net
- Configure the Solver
- caffe train -solver solver.prototxt -gpu 0

先把数据写成caffe的格式，然后设计一个网络，然后用caffe提供的solver来做优化看效果如何。如果你的数据是图像的话，可以从现有的网络，比如说alexnet或者googlenet开始，然后做fine tuning。如果你的数据稍有不同，比如说是直接的float vector，你可能需要做一些个性化的配置。caffe的logistic regression example兴许会很有帮助。

我在和人聊的时候发现大家都比较喜欢fine tune的方法，所以我也简单介绍一下。基本上，fine tuning的想法就是说，我在imagenet那么大的数据集上train好一个很牛的网络了，那别的任务上肯定也不错。所以我可以把pretrain的网络拿过来，然后只重新train最后几层。重新train的意思是说，比如我以前需要分类imagenet的一千类，现在我只想识别是狗还是猫，或者是不是车牌。于是我就可以把最后一层softmax从一个4096*1000的分类器变成一个40962的分类器。这个策略在应用中非常好使，所以我们经常会先在imagenet上pretrain一个网络，因为我们知道imagenet上training的大概过程会怎么样。

## 一些tips

目前deep learning用在小数据集上有什么好的方法吗？在小数据集的问题上是不是可以通过减少网络的层数来减少过拟合？小数据集基本上需要通过小的模型来防止overfit，当然如果数据集是图像等等，也可以通过finetuning。另外一个可能是直接手标更多数据，有时候糙快猛但是还挺好使的。caffe对不同尺度的同一对象的分类和识别有哪些特殊的处理方法？这个倒也不单是caffe的问题，在图像识别上如果需要处理不同尺度，一般就是做multi-scale的detection，可以参考一下selective search，R-CNN等等的工作。

# 部署在iOS上
在iOS上使用caffe，已经有前人做了一些工作，我这里使用的是[这个](https://github.com/solrex/caffe-mobile)，在这里，也对作者[Wenbo Yang](https://yangwenbo.com)表示感谢。

按照作者的步骤，我们先使用下面指令编译整个caffe库：

```
$ git clone --recursive https://github.com/solrex/caffe-mobile.git
$ ./tools/build_ios.sh
```
我们就得到了一个.a的文件，将这个库加入到工程中，然后将caffe的模型.caffemodel文件加入工程中，在工程中使用下面代码调用这个模型得到结果：

初始化网络：
```
Caffe::set_mode(Caffe::CPU);
NetParameter netParam;
LOG(INFO) << "load mode 1";
int fileSize = -1;
const void* fileData = (const void*)ReadAllBytes([FilePathForResourceName(@"xxx", @"caffemodel") UTF8String],&fileSize);
ReadProtoFromArray(fileData,fileSize, &netParam);
delete((char *)fileData);
shared_ptr<Net<float> > net(new Net<float>(netParam));
_net = net;
```

使用到的函数：
```
inline static char * ReadAllBytes(const char * filename, int * read)
{
    ifstream ifs(filename, ios::binary|ios::ate);
    ifstream::pos_type pos = ifs.tellg();
    
    // What happens if the OS supports really big files.
    // It may be larger than 32 bits?
    // This will silently truncate the value/
    int length = (int)pos;
    
    // Manuall memory management.
    // Not a good idea use a container/.
    char *pChars = new char[length];
    ifs.seekg(0, ios::beg);
    ifs.read(pChars, length);
    
    // No need to manually close.
    // When the stream goes out of scope it will close the file
    // automatically. Unless you are checking the close for errors
    // let the destructor do it.
    ifs.close();
    *read = length;
    return pChars;
}

inline static void ReadProtoFromArray(const void* inputData,int length, NetParameter* netParam) {
    netParam->ParseFromArray(inputData,length);
}
```

使用网络预测和读取结果：
```
caffe::Blob<float> *input_layer = _net->input_blobs()[0];
//    NSString *test_file_path = FilePathForResourceName(@"2", @"jpg");
timer.Start();
if(! ReadImageToBlob(test_file_path, input_layer)) {
    LOG(INFO) << "ReadImageToBlob failed";
    return;
}
_net->Forward();
timer.Stop();
LOG(INFO) << "total time:" << timer.MilliSeconds();

//read result
shared_ptr<Blob<float> > a= _net->blob_by_name("prob");
for(int u = 0; u < a->num(); u++)
{
    for(int v = 0; v < a->channels(); v++)
        // for(int v = 1; v < 2; v++)
    {
        for(int w = 0; w < a->height(); w++)
        {
            for(int x = 0; x < a->width(); x++)
            {
                LOG(INFO)<<a->data_at(u, v, w, x);
            }
        }
    }
}
```

## 遇到的问题
由于我们的模型用到了一些自定义的层，第一次运行时不出意料不足为奇的失败了，提示找不到层。于是将自定义的层加入scr中对应的文件夹中，并在caffe.proto中加入对应的参数定义，而后重新编译整个caffe库，问题解决，模型在iPhone中欢快的跑起了，时间也非常快。

下一步的目标是将iPhone中的GPU利用起来。

<!-- markdown end -->
</div>
<div class="entry" id="main">
<!-- content -->
<p>项目是用到一个模型是公司的模型，但是只有一个caffe的版本，正好学习一下caffe的使用。</p>

<h1 id="">简介</h1>

<p>这里主要是对caffe作者的一个<a href="http://on-demand.gputechconf.com/gtc/2014/webinar/gtc-express-deep-learning-caffee-evan-shelhamer.pdf">教程</a>的总结，也参考了<a href="http://www.datakit.cn/blog/2015/06/12/online_meet_up_with_yangqing_jia.html">这篇文章</a>。</p>

<h2 id="caffe">caffe起源</h2>

<p>大家最近一段时间应该已经听到很多关于deep learning的八卦了，deep learning比较流行的一个原因，<strong><em>主要是因为它能够自主地从数据上学到有用的feature</em></strong>,特别是对于一些不知道如何设计feature的场合，比如说图像和speech deep learning可以学习到比以往比如说sift或者MFCC这样手工设计的feature更好的方法, 这些feature有很强的semantic的含义。所以很多时候在用到其他的一些任务的时候会很有效，这也是为什么我们可以用一个feature来实现很多比如说识别，检测，物体分割这样的不同任务的缘故。</p>

<p>无论如何，深度学习其实说回来是个挺久的话题了，Yann Lecun在89年的时候就提出了convolutional Neural Net的想法，然后在手写数字上获得了很大的成功。最近深度学习重新受到关注，最大的原因是两个：</p>

<ul>
<li>一个是大规模的数据集使得我们可以学习到远比digit更加复杂的概念</li>
<li>另外一个是大规模并行计算让我们可以做很快的优化，使得以前我们没法想象的计算量都变成很容易实现的了</li>
</ul>

<p>所以这些都很美好。但是问题是写code还挺麻烦的。所以大家肯定希望有个比较好用的框架来很快上手和试试这些深度学习的算法。 所以这就是Caffe了！Caffe是作者在Berkeley写论文的时候想学习C++和cuda写的，然后写完了觉得我自己用太亏了，所以想贡献给社区让大家来用。所以如果你看见一些写得很烂的code，不要骂作者。</p>

<h2 id="caffe">caffe介绍</h2>

<p>caffe的好处是，我们基本上可以用一个比较简单的语言（google protobuffer）来定义许多网络结构，然后我们可以在CPU或者GPU上面执行这些代码，而且cpu和gpu在数学结果上是兼容的。然后，所有的模型和recipe我们都会公布出来，使得我们可以很容易地reproduce互相发布的结果。这也是我感到很幸运的一个地方，大家都很喜欢caffe，也很喜欢分享自己paper里的成果（比如说MIT的place net和VGG的模型）。</p>

<p>无论如何，这就是Caffe的简单介绍了，最开始是一个hobby project，但是最近Berkeley和其他公司比如说NVidia，Yahoo在很认真地维护它，希望能够把整个架构做的更好用。</p>

<p>然后我大概讲一下caffe的设计吧。</p>

<p>基本上，caffe 遵循了神经网络的一个简单假设 - 所有的计算都是以layer的形式表示的layer做的事情就是拿到一些数据，然后输出一些计算以后的结果，比如说卷积，就是输入一个图像，然后和这一层的参数（filter）做卷积,然后输出卷积的结果。每一个layer需要做两个计算：forward是从输入计算输出，然后backward是从上面给的gradient来计算相对于输入的gradient。只要这两个函数实现了以后，我们就可以把很多层连接成一个网络，这个网络做的事情就是输入我们的数据（图像或者语音或者whatever），然后来计算我们需要的输出（比如说识别的label）。只要这两个函数实现了以后，我们就可以把很多层连接成一个网络，这个网络做的事情就是输入我们的数据（图像或者语音或者whatever），然后来计算我们需要的输出（比如说识别的label）。在训练的时候，我们可以根据已有的label来计算loss和gradient，然后用gradient来update网络的参数。这个就是Caffe的一个基本流程！</p>

<p>如果大家需要自己实现一个layer的话，可以参考下图。比如说输入是x，我们可以想象一个layer的forward function就是y=f(x) 然后，我们会有一个loss function，记成L(.)。在做backward的时候，网络得到的是上层给出的gradient，dL/dy，然后网络需要做的计算是dL/dx = dL/dy * dy/dx，dy/dx也就是f’(x)。于是，这样我们就可以一层一层往后计算gradient。Caffe里面实现的solver主要也是为了神经网络设计的。</p>

<p><img src="https://masstone-phoenix.github.io/image/caffe/caffe_layer.png" alt="" title=""></p>

<p>在做training的时候，我们一般都会做SGD，就是每次输入一个小batch，做计算，update参数，然后再输入下一个batch Caffe也实现了许多实际应用上比简单SGD要更有效的算法，比如说momentum 和Adagrad （顺便插一句，Ilya Sutskever有paper解释说，momemtum其实已经可以很好地实现quasi second order的优化，所以建议大家可以从momentum sgd开始尝试做training）.</p>

<p>基本上，最简单地用caffe上手的方法就是：</p>

<ul>
<li>Convert the data to a Caffe format</li>
<li>lmdb, leveldb, hdf5 / .mat, list of images, etc.</li>
<li>Define the Net</li>
<li>Configure the Solver</li>
<li>caffe train -solver solver.prototxt -gpu 0</li>
</ul>

<p>先把数据写成caffe的格式，然后设计一个网络，然后用caffe提供的solver来做优化看效果如何。如果你的数据是图像的话，可以从现有的网络，比如说alexnet或者googlenet开始，然后做fine tuning。如果你的数据稍有不同，比如说是直接的float vector，你可能需要做一些个性化的配置。caffe的logistic regression example兴许会很有帮助。</p>

<p>我在和人聊的时候发现大家都比较喜欢fine tune的方法，所以我也简单介绍一下。基本上，fine tuning的想法就是说，我在imagenet那么大的数据集上train好一个很牛的网络了，那别的任务上肯定也不错。所以我可以把pretrain的网络拿过来，然后只重新train最后几层。重新train的意思是说，比如我以前需要分类imagenet的一千类，现在我只想识别是狗还是猫，或者是不是车牌。于是我就可以把最后一层softmax从一个4096*1000的分类器变成一个40962的分类器。这个策略在应用中非常好使，所以我们经常会先在imagenet上pretrain一个网络，因为我们知道imagenet上training的大概过程会怎么样。</p>

<h2 id="tips">一些tips</h2>

<p>目前deep learning用在小数据集上有什么好的方法吗？在小数据集的问题上是不是可以通过减少网络的层数来减少过拟合？小数据集基本上需要通过小的模型来防止overfit，当然如果数据集是图像等等，也可以通过finetuning。另外一个可能是直接手标更多数据，有时候糙快猛但是还挺好使的。caffe对不同尺度的同一对象的分类和识别有哪些特殊的处理方法？这个倒也不单是caffe的问题，在图像识别上如果需要处理不同尺度，一般就是做multi-scale的detection，可以参考一下selective search，R-CNN等等的工作。</p>

<h1 id="ios">部署在iOS上</h1>

<p>在iOS上使用caffe，已经有前人做了一些工作，我这里使用的是<a href="https://github.com/solrex/caffe-mobile">这个</a>，在这里，也对作者<a href="https://yangwenbo.com">Wenbo Yang</a>表示感谢。</p>

<p>按照作者的步骤，我们先使用下面指令编译整个caffe库：</p>

<pre><code>$ git clone --recursive https://github.com/solrex/caffe-mobile.git
$ ./tools/build_ios.sh
</code></pre>

<p>我们就得到了一个.a的文件，将这个库加入到工程中，然后将caffe的模型.caffemodel文件加入工程中，在工程中使用下面代码调用这个模型得到结果：</p>

<p>初始化网络：</p>

<pre><code>Caffe::set_mode(Caffe::CPU);
NetParameter netParam;
LOG(INFO) &lt;&lt; "load mode 1";
int fileSize = -1;
const void* fileData = (const void*)ReadAllBytes([FilePathForResourceName(@"xxx", @"caffemodel") UTF8String],&amp;fileSize);
ReadProtoFromArray(fileData,fileSize, &amp;netParam);
delete((char *)fileData);
shared_ptr&lt;Net&lt;float&gt; &gt; net(new Net&lt;float&gt;(netParam));
_net = net;
</code></pre>

<p>使用到的函数：</p>

<pre><code>inline static char * ReadAllBytes(const char * filename, int * read)
{
    ifstream ifs(filename, ios::binary|ios::ate);
    ifstream::pos_type pos = ifs.tellg();

    // What happens if the OS supports really big files.
    // It may be larger than 32 bits?
    // This will silently truncate the value/
    int length = (int)pos;

    // Manuall memory management.
    // Not a good idea use a container/.
    char *pChars = new char[length];
    ifs.seekg(0, ios::beg);
    ifs.read(pChars, length);

    // No need to manually close.
    // When the stream goes out of scope it will close the file
    // automatically. Unless you are checking the close for errors
    // let the destructor do it.
    ifs.close();
    *read = length;
    return pChars;
}

inline static void ReadProtoFromArray(const void* inputData,int length, NetParameter* netParam) {
    netParam-&gt;ParseFromArray(inputData,length);
}
</code></pre>

<p>使用网络预测和读取结果：</p>

<pre><code>caffe::Blob&lt;float&gt; *input_layer = _net-&gt;input_blobs()[0];
//    NSString *test_file_path = FilePathForResourceName(@"2", @"jpg");
timer.Start();
if(! ReadImageToBlob(test_file_path, input_layer)) {
    LOG(INFO) &lt;&lt; "ReadImageToBlob failed";
    return;
}
_net-&gt;Forward();
timer.Stop();
LOG(INFO) &lt;&lt; "total time:" &lt;&lt; timer.MilliSeconds();

//read result
shared_ptr&lt;Blob&lt;float&gt; &gt; a= _net-&gt;blob_by_name("prob");
for(int u = 0; u &lt; a-&gt;num(); u++)
{
    for(int v = 0; v &lt; a-&gt;channels(); v++)
        // for(int v = 1; v &lt; 2; v++)
    {
        for(int w = 0; w &lt; a-&gt;height(); w++)
        {
            for(int x = 0; x &lt; a-&gt;width(); x++)
            {
                LOG(INFO)&lt;&lt;a-&gt;data_at(u, v, w, x);
            }
        }
    }
}
</code></pre>

<h2 id="">遇到的问题</h2>

<p>由于我们的模型用到了一些自定义的层，第一次运行时不出意料不足为奇的失败了，提示找不到层。于是将自定义的层加入scr中对应的文件夹中，并在caffe.proto中加入对应的参数定义，而后重新编译整个caffe库，问题解决，模型在iPhone中欢快的跑起了，时间也非常快。</p>

<p>下一步的目标是将iPhone中的GPU利用起来。</p>
<!-- content end -->
</div>
<br>
<br>
    <div id="disqus_thread"></div>
	<div class="footer">
		<p>© Copyright 2014 by isnowfy, Designed by isnowfy</p>
	</div>
</div>
<script src="main.js"></script>
<script id="content" type="text/mustache">
    <h1>{{title}}</h1>
    <div class="tag">
    {{date}}
    {{#tags}}
    <a href="/#/tag/{{name}}">#{{name}}</a>
    {{/tags}}
    </div>
</script>
<script id="pagesTemplate" type="text/mustache">
    {{#pages}}
    <li>
        <a href="{{path}}">{{title}}</a>
    </li>
    {{/pages}}
</script>
<script>
$(document).ready(function() {
    $.ajax({
        url: "main.json",
        type: "GET",
        dataType: "json",
        success: function(data) {
            $("#title").html(data.name);
            var pagesTemplate = Hogan.compile($("#pagesTemplate").html());
            var pagesHtml = pagesTemplate.render({"pages": data.pages});
            $("#pages").append(pagesHtml);
            //path
            var path = "2018/caffe_usage.html";
            //path end
            var now = 0;
            for (var i = 0; i < data.posts.length; ++i)
                if (path == data.posts[i].path)
                    now = i;
            var post = data.posts[now];
            var tmp = post.tags.split(" ");
            var tags = [];
            for (var i = 0; i < tmp.length; ++i)
                if (tmp[i].length > 0)
                    tags.push({"name": tmp[i]});
            var contentTemplate = Hogan.compile($("#content").html());
            var contentHtml = contentTemplate.render({"title": post.title, "tags": tags, "date": post.date});
            $("#main").prepend(contentHtml);
            if (data.disqus_shortname.length > 0) {
                var disqus_shortname = data.disqus_shortname;
                (function() {
                    var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
                    dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
                    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
                })();
            }
        }
    });
});
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ["\\(", "\\)"]], processEscapes: true}});
</script>
</body>
</html>
