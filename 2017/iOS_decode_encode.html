<!DOCTYPE HTML>
<html>
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
    <meta name="Keywords" content="blog"/>
    <meta name="Description" content="blog"/>
    <title>Simple</title>
    <link rel="shortcut icon" href="/static/favicon.png"/>
    <link rel="stylesheet" type="text/css" href="/main.css" />
</head>
<body>
<div class="main">
    <div class="header">
    	<ul id="pages">
            <li><a href="/">home</a></li>
            <li><a href="/#/tags">tags</a></li>
            <li><a href="/#/archive">archive</a></li>
    	</ul>
    </div>
	<div class="wrap-header">
	<h1>
    <a href="/" id="title"></a>
	</h1>
	</div>
<div id="md" style="display: none;">
<!-- markdown -->
这是WWDC2014 513中学习如何硬编解码视频的总结。

iOS系统上视频的处理由顶层到底层分别是AVKit->AVFoundation->Video Toolbox->Core Media->Core Video。

其中Core Video提供了处理视频的一个流水线模型。

![](https://masstone-phoenix.github.io/image/iOS_h264_wwdc2014_513/corevideo_pipeline_2x.png)

其中有一些重要的概念，如：

- Display Link：其实是一个特殊的定时器，在一个独立的高优先级的线程中运行，负责显示的刷新
- Buffer Management：比如CVPixelBuffer是在主内存中的图像，其层次结构从高到低为CVBuffer->CVImageBuffer->CVPixelBuffer；以及一些其它的buffer，比如Core Video OpenGL buffer、Core Video OpenGL texture ；最后，buffer经常使用buffer pool进行管理。


![](https://masstone-phoenix.github.io/image/iOS_h264_wwdc2014_513/obtaining_frames_2x.png)

#用到的数据类型

##CVPixelBuffer
存放已解压缩的光栅数据。

![](https://masstone-phoenix.github.io/image/iOS_h264_wwdc2014_513/CVPixelBuffer.png)

##CVPixelBufferPool
![](https://masstone-phoenix.github.io/image/iOS_h264_wwdc2014_513/CVPixelBufferPool.png)

##pixelBufferAttributes
宽、高，像素格式，兼容性等信息。

##CMTime
包括64-bit Time Value (Numerator)和32-bit Time Scale (Denominator)。表示 value/timescale 秒 。
参考<http://stackoverflow.com/questions/12902410/trying-to-understand-cmtime>。

##CMVideoFormatDescription
描述视频格式的类型。

##CMBlockBuffer
可以包含你任意数据。在视频处理的一般情况下，包含压缩的数据。

##CMSampleBuffer
![](https://masstone-phoenix.github.io/image/iOS_h264_wwdc2014_513/CMSampleBuffer.png)

##CMClock
只有一个读取函数CMClockGetHostTimeClock()；它通过 ```mach_absolute_time()```实现。它在设备休眠时会暂停。

##CMTimebase
我们改变媒体时间的接口，内部通过CMClock实现。

剩下主要介绍AVFoundation和Video Toolbox。

#AVFoundation
AVFoundation主要作用是直接显示码流、将数据直接编码成文件。

##Export and Trimming
```
AVAssetExportSession *exportSession = [[AVAssetExportSession alloc]
initWithAsset:asset presetName:AVAssetExportPresetMediumQuality];
exportSession.outputURL = ...;
exportSession.outputFileType = AVFileTypeQuickTimeMovie;
exportSession.timeRange = CMTimeRangeMake(startTime, duration);
exportSession.metadata = ...;
[exportSession exportAsynchronouslyWithCompletionHandler:handlerBlock];

void (^handlerBlock)(void) = ^{
    switch (exportSession.status) {
        case AVAssetExportSessionStatusCompleted:
            /* export complete */
            break;
        case AVAssetExportSessionStatusFailed:
            /* export error (see exportSession.error) */
            break;
        case AVAssetExportSessionStatusCancelled:
            /* export cancelled */
            break;
    }
}

```
需要注意：

- AVAssetExportSession will not overwrite files
- AVAssetExportSession will not write files outside of your sandbox

##使用AVComposition剪辑视频
- 剪辑一个composition的所有track：
```
[composition insertTimeRange:... ofAsset:... atTime:... error:...]
```
- 剪辑单个视频：
```
[compositionTrack insertTimeRange:... ofTrack:... atTime:... error:...];
[compositionTrack insertTimeRanges:... ofTracks:... atTime:... error:...];
```
- 直接编辑segment array：
```
[compositionTrack setSegments:...];
```

e.g.：
```
AVMutableComposition *composition = [AVMutableComposition composition];
AVMutableCompositionTrack *compositionVideoTrack =
    [composition addMutableTrackWithMediaType:AVMediaTypeVideo
                                                    preferredTrackID:...];
[compositionVideoTrack insertTimeRange:... ofTrack:clipVideoTrack atTime:... error:...];
```

注意：不要在播放，生成图片、export或读取AVMutableComposition时修改它。make a copy。

##混音
使用AVAudioMix对音频track进行混音，它有一个AVMutableAudioMixInputParameters的数组，通过这个数组的输入对音频进行混音。

```
AVMutableAudioMixInputParameters *trackMix =
    [AVMutableAudioMixInputParameters
        audioMixInputParametersWithTrack:mainAudioTrack];
[trackMix setVolume:1.0 atTime:kCMTimeZero];

[trackMix setVolumeRampFromStartVolume:1.0
        toEndVolume:0.2
        timeRange:CMTimeRangeMake(x,y-x)];
...

AVMutableAudioMix *audioMix = [AVMutableAudioMix audioMix];
audioMix.inputParameters = [NSArray arrayWithObject:trackMix];


Using AVAudioMix
• To apply AVAudioMix for playback:
    playerItem.audioMix = audioMix;
• To apply AVAudioMix for export:
    exportSession.audioMix = audioMix;
• To apply AVAudioMix for sample reading:
    assetReaderAudioMixOutput.audioMix = audioMix;

```

##视频过渡动画
AVMutableVideoComposition有一个内容为AVVideoCompositionInstruction的数组来进行视频处理。通过AVVideoCompositionInstruction，我们可以改变视频的背景色，使用后处理或应用图层指令（AVMutableVideoCompositionLayerInstruction）。

使用AVMutableVideoCompositionLayerInstruction，我们可以应用透明度、transform等。

```
AVAsset *firstVideoAssetTrack = <#AVAssetTrack representing the first video segment played in the composition#>;
AVAsset *secondVideoAssetTrack = <#AVAssetTrack representing the second video segment played in the composition#>;
// Create the first video composition instruction.
AVMutableVideoCompositionInstruction *firstVideoCompositionInstruction = [AVMutableVideoCompositionInstruction videoCompositionInstruction];
// Set its time range to span the duration of the first video track.
firstVideoCompositionInstruction.timeRange = CMTimeRangeMake(kCMTimeZero, firstVideoAssetTrack.timeRange.duration);
// Create the layer instruction and associate it with the composition video track.
AVMutableVideoCompositionLayerInstruction *firstVideoLayerInstruction = [AVMutableVideoCompositionLayerInstruction videoCompositionLayerInstructionWithAssetTrack:mutableCompositionVideoTrack];
// Create the opacity ramp to fade out the first video track over its entire duration.
[firstVideoLayerInstruction setOpacityRampFromStartOpacity:1.f toEndOpacity:0.f timeRange:CMTimeRangeMake(kCMTimeZero, firstVideoAssetTrack.timeRange.duration)];
// Create the second video composition instruction so that the second video track isn't transparent.
AVMutableVideoCompositionInstruction *secondVideoCompositionInstruction = [AVMutableVideoCompositionInstruction videoCompositionInstruction];
// Set its time range to span the duration of the second video track.
secondVideoCompositionInstruction.timeRange = CMTimeRangeMake(firstVideoAssetTrack.timeRange.duration, CMTimeAdd(firstVideoAssetTrack.timeRange.duration, secondVideoAssetTrack.timeRange.duration));
// Create the second layer instruction and associate it with the composition video track.
AVMutableVideoCompositionLayerInstruction *secondVideoLayerInstruction = [AVMutableVideoCompositionLayerInstruction videoCompositionLayerInstructionWithAssetTrack:mutableCompositionVideoTrack];
// Attach the first layer instruction to the first video composition instruction.
firstVideoCompositionInstruction.layerInstructions = @[firstVideoLayerInstruction];
// Attach the second layer instruction to the second video composition instruction.
secondVideoCompositionInstruction.layerInstructions = @[secondVideoLayerInstruction];
// Attach both of the video composition instructions to the video composition.
AVMutableVideoComposition *mutableVideoComposition = [AVMutableVideoComposition videoComposition];
mutableVideoComposition.instructions = @[firstVideoCompositionInstruction, secondVideoCompositionInstruction];



使用AVVideoComposition
• For playback:
    playerItem.videoComposition = videoComposition;
• For image generation:
    assetImageGenerator.videoComposition = videoComposition;
• For export:
    assetExportSession.videoComposition = videoComposition;
• To retrieve rendered frames:
    assetReaderVideoCompositionOutput.videoComposition
                                                            = videoComposition;

```


##读写音视频文件

###读文件
![](https://masstone-phoenix.github.io/image/iOS_h264_wwdc2014_513/AVAssetReader.png)
```
//Instantiate asset reader
AVAssetReader *assetReader = [AVAssetReader assetReaderWithAsset:asset
    error:&error];
//Add outputs
AVAssetReaderOutput *trackOutput = [AVAssetReaderTrackOutput
    assetReaderTrackOutputWithTrack:audioTrack
    outputSettings:outputSettings];
[assetReader addOutput:trackOutput];
//Configure
assetReader.timeRange = CMTimeRangeMake(kCMTimeZero, CMTimeMake(5, 1));
//Start reading
BOOL success = [assetReader startReading];

BOOL done = NO;
while (!done) {
    CMSampleBufferRef sampleBuffer = [trackOutput copyNextSampleBuffer];
    if (sampleBuffer) {
        // Extract & draw waveform sample values
        CFRelease(sampleBuffer);
    } else {
        AVAssetReaderStatus status = assetReader.status;
        // act on asset reader status
        done = YES;
    }
}
```

###写文件
![](https://masstone-phoenix.github.io/image/iOS_h264_wwdc2014_513/AVAssetWriter.png)
```
//Instantiate asset writer
AVAssetWriter *assetWriter = [AVAssetWriter
    assetWriterWithURL:localOutputURL fileType:AVFileTypeQuickTimeMovie
    error:&localError];
//Add inputs
AVAssetWriterInput *videoInput =
    [AVAssetWriterInput assetWriterInputWithMediaType:AVMediaTypeVideo
    outputSettings:compressionVideoSettings];
    [assetWriter addInput:videoInput];
//Configure
assetWriter.shouldOptimizeForNetworkUse = YES;
//Start writing
BOOL success = [assetWriter startWriting];

//Start session
[assetWriter startSessionAtSourceTime:kCMTimeZero];
//Append samples
CMSampleBufferRef sampleBuffer = ...;
success = [videoInput appendSampleBuffer:sampleBuffer];
//Finish writing
success = [assetWriter finishWriting];
if (!success) {
    NSError *error = assetWriter.error;
    // deal with error
}

```

####音频视频交织

1. pull model

```
[myAVAssetWriterInput requestMediaDataWhenReadyOnQueue:queue usingBlock:^{
    while (myAVAssetWriterInput.readyForMoreMediaData) {
        CMSampleBufferRef sampleBuffer = ...;
        if (sampleBuffer) {
            [myAVAssetWriterInput appendSampleBuffer:sampleBuffer];
            CFRelease(sampleBuffer);
         } else {
            [myAVAssetWriterInput markAsFinished];
            break;
        }
     }
}];
```
2. push model

```
- captureOutput:... didOutputSampleBuffer:sampleBuffer fromConnection:...
{
    if (assetWriterInput.readyForMoreMediaData) {
        // perform real-time processing
    [assetWriterInput appendSampleBuffer:sampleBuffer];
    } else {
        // drop video frames, potentially throttle upstream
        // Avoid queueing, if possible
    }
}
```


#Video Toolbox
Video Toolbox的主要作用是解码流解压到CVPixelBuffer中、将原始数据编码到CMSampleBuffer中。

##编码
使用VTCompressionSession进行视频的编码。框架如下：

![](https://masstone-phoenix.github.io/image/iOS_h264_wwdc2014_513/VTCompressionSession.png)

###创建一个VTCompressionSession
需要视频的宽高、编码格式、VTCompressionOutputCallback和可选的sourceImageBufferAttributes。
```
OSStatus VTCompressionSessionCreate(CFAllocatorRef allocator, int32_t width, int32_t height, CMVideoCodecType codecType, CFDictionaryRef encoderSpecification, CFDictionaryRef sourceImageBufferAttributes, CFAllocatorRef compressedDataAllocator, VTCompressionOutputCallback outputCallback, void *outputCallbackRefCon, VTCompressionSessionRef  _Nullable *compressionSessionOut);
```

###配置VTCompressionSession
使用VTSessionSetProperty()配置VTCompressionSession。

```
kVTCompressionPropertyKey_AllowFrameReordering：允许调整帧的顺序
kVTCompressionPropertyKey_AverageBitRate：设置平均码率
kVTCompressionPropertyKey_H264EntropyMode（kVTH264EntropyMode_CAVLC/kVTH264EntropyMode_CABAC）：H264熵编码的算法，一般来说，CABAC压缩率更高但计算量更大
kVTCompressionPropertyKey_RealTime：是否实时
kVTCompressionPropertyKey_ProfileLevel：选择preset等级，比如kVTProfileLevel_H264_Main_AutoLevel
```

###开始编码
使用```err = VTCompressionSessionEncodeFrame( session, pixelBuffer,presentationTime, … );```对帧进行编码，需要按***显示顺序（Presentation order）***提供给VTCompressionSession，输出可能会有延时，使用```VTCompressionSessionCompleteFrames()```来flush编码器。

###处理编码结果（CMSampleBuffer）
当编码器有输出时，会调用VTCompressionOutputCallback，它会提供：

- CMSampleBuffer
- 压缩错误码
- 掉帧

***需要说明的是，编码输出的顺序是decode order***

###将编码得到的数据转化为基本流（Elementary Stream）
以下都是针对H264：

![](https://masstone-phoenix.github.io/image/iOS_h264_wwdc2014_513/ConversionOfPP.png)

还需要将MPEG4的Header转换成NALU的3/4-byte开始码。
##解码





<!-- markdown end -->
</div>
<div class="entry" id="main">
<!-- content -->
<p>这是WWDC2014 513中学习如何硬编解码视频的总结。</p>

<p>iOS系统上视频的处理由顶层到底层分别是AVKit-&gt;AVFoundation-&gt;Video Toolbox-&gt;Core Media-&gt;Core Video。</p>

<p>其中Core Video提供了处理视频的一个流水线模型。</p>

<p><img src="https://masstone-phoenix.github.io/image/iOS_h264_wwdc2014_513/corevideo_pipeline_2x.png" alt="" title=""></p>

<p>其中有一些重要的概念，如：</p>

<ul>
<li>Display Link：其实是一个特殊的定时器，在一个独立的高优先级的线程中运行，负责显示的刷新</li>
<li>Buffer Management：比如CVPixelBuffer是在主内存中的图像，其层次结构从高到低为CVBuffer-&gt;CVImageBuffer-&gt;CVPixelBuffer；以及一些其它的buffer，比如Core Video OpenGL buffer、Core Video OpenGL texture ；最后，buffer经常使用buffer pool进行管理。</li>
</ul>

<p><img src="https://masstone-phoenix.github.io/image/iOS_h264_wwdc2014_513/obtaining_frames_2x.png" alt="" title=""></p>

<h1 id="">用到的数据类型</h1>

<h2 id="cvpixelbuffer">CVPixelBuffer</h2>

<p>存放已解压缩的光栅数据。</p>

<p><img src="https://masstone-phoenix.github.io/image/iOS_h264_wwdc2014_513/CVPixelBuffer.png" alt="" title=""></p>

<h2 id="cvpixelbufferpool">CVPixelBufferPool</h2>

<p><img src="https://masstone-phoenix.github.io/image/iOS_h264_wwdc2014_513/CVPixelBufferPool.png" alt="" title=""></p>

<h2 id="pixelbufferattributes">pixelBufferAttributes</h2>

<p>宽、高，像素格式，兼容性等信息。</p>

<h2 id="cmtime">CMTime</h2>

<p>包括64-bit Time Value (Numerator)和32-bit Time Scale (Denominator)。表示 value/timescale 秒 。
参考<a href="http://stackoverflow.com/questions/12902410/trying-to-understand-cmtime">http://stackoverflow.com/questions/12902410/trying-to-understand-cmtime</a>。</p>

<h2 id="cmvideoformatdescription">CMVideoFormatDescription</h2>

<p>描述视频格式的类型。</p>

<h2 id="cmblockbuffer">CMBlockBuffer</h2>

<p>可以包含你任意数据。在视频处理的一般情况下，包含压缩的数据。</p>

<h2 id="cmsamplebuffer">CMSampleBuffer</h2>

<p><img src="https://masstone-phoenix.github.io/image/iOS_h264_wwdc2014_513/CMSampleBuffer.png" alt="" title=""></p>

<h2 id="cmclock">CMClock</h2>

<p>只有一个读取函数CMClockGetHostTimeClock()；它通过 <code>mach_absolute_time()</code>实现。它在设备休眠时会暂停。</p>

<h2 id="cmtimebase">CMTimebase</h2>

<p>我们改变媒体时间的接口，内部通过CMClock实现。</p>

<p>剩下主要介绍AVFoundation和Video Toolbox。</p>

<h1 id="avfoundation">AVFoundation</h1>

<p>AVFoundation主要作用是直接显示码流、将数据直接编码成文件。</p>

<h2 id="exportandtrimming">Export and Trimming</h2>

<pre><code>AVAssetExportSession *exportSession = [[AVAssetExportSession alloc]
initWithAsset:asset presetName:AVAssetExportPresetMediumQuality];
exportSession.outputURL = ...;
exportSession.outputFileType = AVFileTypeQuickTimeMovie;
exportSession.timeRange = CMTimeRangeMake(startTime, duration);
exportSession.metadata = ...;
[exportSession exportAsynchronouslyWithCompletionHandler:handlerBlock];

void (^handlerBlock)(void) = ^{
    switch (exportSession.status) {
        case AVAssetExportSessionStatusCompleted:
            /* export complete */
            break;
        case AVAssetExportSessionStatusFailed:
            /* export error (see exportSession.error) */
            break;
        case AVAssetExportSessionStatusCancelled:
            /* export cancelled */
            break;
    }
}
</code></pre>

<p>需要注意：</p>

<ul>
<li>AVAssetExportSession will not overwrite files</li>
<li>AVAssetExportSession will not write files outside of your sandbox</li>
</ul>

<h2 id="avcomposition">使用AVComposition剪辑视频</h2>

<ul>
<li>剪辑一个composition的所有track：</li>
</ul>

<pre><code>[composition insertTimeRange:... ofAsset:... atTime:... error:...]
</code></pre>

<ul>
<li>剪辑单个视频：</li>
</ul>

<pre><code>[compositionTrack insertTimeRange:... ofTrack:... atTime:... error:...];
[compositionTrack insertTimeRanges:... ofTracks:... atTime:... error:...];
</code></pre>

<ul>
<li>直接编辑segment array：</li>
</ul>

<pre><code>[compositionTrack setSegments:...];
</code></pre>

<p>e.g.：</p>

<pre><code>AVMutableComposition *composition = [AVMutableComposition composition];
AVMutableCompositionTrack *compositionVideoTrack =
    [composition addMutableTrackWithMediaType:AVMediaTypeVideo
                                                    preferredTrackID:...];
[compositionVideoTrack insertTimeRange:... ofTrack:clipVideoTrack atTime:... error:...];
</code></pre>

<p>注意：不要在播放，生成图片、export或读取AVMutableComposition时修改它。make a copy。</p>

<h2 id="">混音</h2>

<p>使用AVAudioMix对音频track进行混音，它有一个AVMutableAudioMixInputParameters的数组，通过这个数组的输入对音频进行混音。</p>

<pre><code>AVMutableAudioMixInputParameters *trackMix =
    [AVMutableAudioMixInputParameters
        audioMixInputParametersWithTrack:mainAudioTrack];
[trackMix setVolume:1.0 atTime:kCMTimeZero];

[trackMix setVolumeRampFromStartVolume:1.0
        toEndVolume:0.2
        timeRange:CMTimeRangeMake(x,y-x)];
...

AVMutableAudioMix *audioMix = [AVMutableAudioMix audioMix];
audioMix.inputParameters = [NSArray arrayWithObject:trackMix];


Using AVAudioMix
• To apply AVAudioMix for playback:
    playerItem.audioMix = audioMix;
• To apply AVAudioMix for export:
    exportSession.audioMix = audioMix;
• To apply AVAudioMix for sample reading:
    assetReaderAudioMixOutput.audioMix = audioMix;
</code></pre>

<h2 id="">视频过渡动画</h2>

<p>AVMutableVideoComposition有一个内容为AVVideoCompositionInstruction的数组来进行视频处理。通过AVVideoCompositionInstruction，我们可以改变视频的背景色，使用后处理或应用图层指令（AVMutableVideoCompositionLayerInstruction）。</p>

<p>使用AVMutableVideoCompositionLayerInstruction，我们可以应用透明度、transform等。</p>

<pre><code>AVAsset *firstVideoAssetTrack = &lt;#AVAssetTrack representing the first video segment played in the composition#&gt;;
AVAsset *secondVideoAssetTrack = &lt;#AVAssetTrack representing the second video segment played in the composition#&gt;;
// Create the first video composition instruction.
AVMutableVideoCompositionInstruction *firstVideoCompositionInstruction = [AVMutableVideoCompositionInstruction videoCompositionInstruction];
// Set its time range to span the duration of the first video track.
firstVideoCompositionInstruction.timeRange = CMTimeRangeMake(kCMTimeZero, firstVideoAssetTrack.timeRange.duration);
// Create the layer instruction and associate it with the composition video track.
AVMutableVideoCompositionLayerInstruction *firstVideoLayerInstruction = [AVMutableVideoCompositionLayerInstruction videoCompositionLayerInstructionWithAssetTrack:mutableCompositionVideoTrack];
// Create the opacity ramp to fade out the first video track over its entire duration.
[firstVideoLayerInstruction setOpacityRampFromStartOpacity:1.f toEndOpacity:0.f timeRange:CMTimeRangeMake(kCMTimeZero, firstVideoAssetTrack.timeRange.duration)];
// Create the second video composition instruction so that the second video track isn't transparent.
AVMutableVideoCompositionInstruction *secondVideoCompositionInstruction = [AVMutableVideoCompositionInstruction videoCompositionInstruction];
// Set its time range to span the duration of the second video track.
secondVideoCompositionInstruction.timeRange = CMTimeRangeMake(firstVideoAssetTrack.timeRange.duration, CMTimeAdd(firstVideoAssetTrack.timeRange.duration, secondVideoAssetTrack.timeRange.duration));
// Create the second layer instruction and associate it with the composition video track.
AVMutableVideoCompositionLayerInstruction *secondVideoLayerInstruction = [AVMutableVideoCompositionLayerInstruction videoCompositionLayerInstructionWithAssetTrack:mutableCompositionVideoTrack];
// Attach the first layer instruction to the first video composition instruction.
firstVideoCompositionInstruction.layerInstructions = @[firstVideoLayerInstruction];
// Attach the second layer instruction to the second video composition instruction.
secondVideoCompositionInstruction.layerInstructions = @[secondVideoLayerInstruction];
// Attach both of the video composition instructions to the video composition.
AVMutableVideoComposition *mutableVideoComposition = [AVMutableVideoComposition videoComposition];
mutableVideoComposition.instructions = @[firstVideoCompositionInstruction, secondVideoCompositionInstruction];



使用AVVideoComposition
• For playback:
    playerItem.videoComposition = videoComposition;
• For image generation:
    assetImageGenerator.videoComposition = videoComposition;
• For export:
    assetExportSession.videoComposition = videoComposition;
• To retrieve rendered frames:
    assetReaderVideoCompositionOutput.videoComposition
                                                            = videoComposition;
</code></pre>

<h2 id="">读写音视频文件</h2>

<h3 id="">读文件</h3>

<p><img src="https://masstone-phoenix.github.io/image/iOS_h264_wwdc2014_513/AVAssetReader.png" alt="" title=""></p>

<pre><code>//Instantiate asset reader
AVAssetReader *assetReader = [AVAssetReader assetReaderWithAsset:asset
    error:&amp;error];
//Add outputs
AVAssetReaderOutput *trackOutput = [AVAssetReaderTrackOutput
    assetReaderTrackOutputWithTrack:audioTrack
    outputSettings:outputSettings];
[assetReader addOutput:trackOutput];
//Configure
assetReader.timeRange = CMTimeRangeMake(kCMTimeZero, CMTimeMake(5, 1));
//Start reading
BOOL success = [assetReader startReading];

BOOL done = NO;
while (!done) {
    CMSampleBufferRef sampleBuffer = [trackOutput copyNextSampleBuffer];
    if (sampleBuffer) {
        // Extract &amp; draw waveform sample values
        CFRelease(sampleBuffer);
    } else {
        AVAssetReaderStatus status = assetReader.status;
        // act on asset reader status
        done = YES;
    }
}
</code></pre>

<h3 id="">写文件</h3>

<p><img src="https://masstone-phoenix.github.io/image/iOS_h264_wwdc2014_513/AVAssetWriter.png" alt="" title=""></p>

<pre><code>//Instantiate asset writer
AVAssetWriter *assetWriter = [AVAssetWriter
    assetWriterWithURL:localOutputURL fileType:AVFileTypeQuickTimeMovie
    error:&amp;localError];
//Add inputs
AVAssetWriterInput *videoInput =
    [AVAssetWriterInput assetWriterInputWithMediaType:AVMediaTypeVideo
    outputSettings:compressionVideoSettings];
    [assetWriter addInput:videoInput];
//Configure
assetWriter.shouldOptimizeForNetworkUse = YES;
//Start writing
BOOL success = [assetWriter startWriting];

//Start session
[assetWriter startSessionAtSourceTime:kCMTimeZero];
//Append samples
CMSampleBufferRef sampleBuffer = ...;
success = [videoInput appendSampleBuffer:sampleBuffer];
//Finish writing
success = [assetWriter finishWriting];
if (!success) {
    NSError *error = assetWriter.error;
    // deal with error
}
</code></pre>

<h4 id="">音频视频交织</h4>

<ol>
<li>pull model</li>
</ol>

<pre><code>[myAVAssetWriterInput requestMediaDataWhenReadyOnQueue:queue usingBlock:^{
    while (myAVAssetWriterInput.readyForMoreMediaData) {
        CMSampleBufferRef sampleBuffer = ...;
        if (sampleBuffer) {
            [myAVAssetWriterInput appendSampleBuffer:sampleBuffer];
            CFRelease(sampleBuffer);
         } else {
            [myAVAssetWriterInput markAsFinished];
            break;
        }
     }
}];
</code></pre>

<ol>
<li>push model</li>
</ol>

<pre><code>- captureOutput:... didOutputSampleBuffer:sampleBuffer fromConnection:...
{
    if (assetWriterInput.readyForMoreMediaData) {
        // perform real-time processing
    [assetWriterInput appendSampleBuffer:sampleBuffer];
    } else {
        // drop video frames, potentially throttle upstream
        // Avoid queueing, if possible
    }
}
</code></pre>

<h1 id="videotoolbox">Video Toolbox</h1>

<p>Video Toolbox的主要作用是解码流解压到CVPixelBuffer中、将原始数据编码到CMSampleBuffer中。</p>

<h2 id="">编码</h2>

<p>使用VTCompressionSession进行视频的编码。框架如下：</p>

<p><img src="https://masstone-phoenix.github.io/image/iOS_h264_wwdc2014_513/VTCompressionSession.png" alt="" title=""></p>

<h3 id="vtcompressionsession">创建一个VTCompressionSession</h3>

<p>需要视频的宽高、编码格式、VTCompressionOutputCallback和可选的sourceImageBufferAttributes。</p>

<pre><code>OSStatus VTCompressionSessionCreate(CFAllocatorRef allocator, int32_t width, int32_t height, CMVideoCodecType codecType, CFDictionaryRef encoderSpecification, CFDictionaryRef sourceImageBufferAttributes, CFAllocatorRef compressedDataAllocator, VTCompressionOutputCallback outputCallback, void *outputCallbackRefCon, VTCompressionSessionRef  _Nullable *compressionSessionOut);
</code></pre>

<h3 id="vtcompressionsession">配置VTCompressionSession</h3>

<p>使用VTSessionSetProperty()配置VTCompressionSession。</p>

<pre><code>kVTCompressionPropertyKey_AllowFrameReordering：允许调整帧的顺序
kVTCompressionPropertyKey_AverageBitRate：设置平均码率
kVTCompressionPropertyKey_H264EntropyMode（kVTH264EntropyMode_CAVLC/kVTH264EntropyMode_CABAC）：H264熵编码的算法，一般来说，CABAC压缩率更高但计算量更大
kVTCompressionPropertyKey_RealTime：是否实时
kVTCompressionPropertyKey_ProfileLevel：选择preset等级，比如kVTProfileLevel_H264_Main_AutoLevel
</code></pre>

<h3 id="">开始编码</h3>

<p>使用<code>err = VTCompressionSessionEncodeFrame( session, pixelBuffer,presentationTime, … );</code>对帧进行编码，需要按<strong><em>显示顺序（Presentation order）</em></strong>提供给VTCompressionSession，输出可能会有延时，使用<code>VTCompressionSessionCompleteFrames()</code>来flush编码器。</p>

<h3 id="cmsamplebuffer">处理编码结果（CMSampleBuffer）</h3>

<p>当编码器有输出时，会调用VTCompressionOutputCallback，它会提供：</p>

<ul>
<li>CMSampleBuffer</li>
<li>压缩错误码</li>
<li>掉帧</li>
</ul>

<p><strong><em>需要说明的是，编码输出的顺序是decode order</em></strong></p>

<h3 id="elementarystream">将编码得到的数据转化为基本流（Elementary Stream）</h3>

<p>以下都是针对H264：</p>

<p><img src="https://masstone-phoenix.github.io/image/iOS_h264_wwdc2014_513/ConversionOfPP.png" alt="" title=""></p>

<p>还需要将MPEG4的Header转换成NALU的3/4-byte开始码。</p>

<h2 id="">解码</h2>
<!-- content end -->
</div>
<br>
<br>
    <div id="disqus_thread"></div>
	<div class="footer">
		<p>© Copyright 2014 by isnowfy, Designed by isnowfy</p>
	</div>
</div>
<script src="main.js"></script>
<script id="content" type="text/mustache">
    <h1>{{title}}</h1>
    <div class="tag">
    {{date}}
    {{#tags}}
    <a href="/#/tag/{{name}}">#{{name}}</a>
    {{/tags}}
    </div>
</script>
<script id="pagesTemplate" type="text/mustache">
    {{#pages}}
    <li>
        <a href="{{path}}">{{title}}</a>
    </li>
    {{/pages}}
</script>
<script>
$(document).ready(function() {
    $.ajax({
        url: "main.json",
        type: "GET",
        dataType: "json",
        success: function(data) {
            $("#title").html(data.name);
            var pagesTemplate = Hogan.compile($("#pagesTemplate").html());
            var pagesHtml = pagesTemplate.render({"pages": data.pages});
            $("#pages").append(pagesHtml);
            //path
            var path = "2017/iOS_decode_encode.html";
            //path end
            var now = 0;
            for (var i = 0; i < data.posts.length; ++i)
                if (path == data.posts[i].path)
                    now = i;
            var post = data.posts[now];
            var tmp = post.tags.split(" ");
            var tags = [];
            for (var i = 0; i < tmp.length; ++i)
                if (tmp[i].length > 0)
                    tags.push({"name": tmp[i]});
            var contentTemplate = Hogan.compile($("#content").html());
            var contentHtml = contentTemplate.render({"title": post.title, "tags": tags, "date": post.date});
            $("#main").prepend(contentHtml);
            if (data.disqus_shortname.length > 0) {
                var disqus_shortname = data.disqus_shortname;
                (function() {
                    var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
                    dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
                    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
                })();
            }
        }
    });
});
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ["\\(", "\\)"]], processEscapes: true}});
</script>
</body>
</html>
